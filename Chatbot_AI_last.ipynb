{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9v4VwOHdDBNH",
        "outputId": "bbb0bf76-cda3-4e03-c3d0-2b136782845c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Library berhasil diimport\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pickle\n",
        "import os\n",
        "from sentence_transformers import InputExample, losses\n",
        "from torch.utils.data import DataLoader\n",
        "import random\n",
        "import google.generativeai as genai\n",
        "from googletrans import Translator, LANGUAGES\n",
        "from langdetect import detect\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Library berhasil diimport\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoL8eZ0wKl2o",
        "outputId": "1495e9c3-4f9d-4741-ec62-aa4507a69f20"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK dependencies berhasil didownload\n"
          ]
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "print(\"NLTK dependencies berhasil didownload\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1wT-vK3KrME",
        "outputId": "d31571ff-1289-4095-e7a3-6583c7f69402"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Block 3: Dataset berhasil dimuat\n",
            "Jumlah intent: 192\n",
            "Intent 1: abstraction\n",
            "Patterns: 3\n",
            "Responses: 1\n",
            "\n",
            "Intent 2: error\n",
            "Patterns: 3\n",
            "Responses: 1\n",
            "\n",
            "Intent 3: documentation\n",
            "Patterns: 3\n",
            "Responses: 1\n",
            "\n",
            "Intent 4: testing\n",
            "Patterns: 1\n",
            "Responses: 1\n",
            "\n",
            "Intent 5: datastructure\n",
            "Patterns: 1\n",
            "Responses: 1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def load_dataset(file_path):\n",
        "    \"\"\"Load dataset dari file JSON\"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            data = json.load(file)\n",
        "        return data\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File {file_path} tidak ditemukan\")\n",
        "        return None\n",
        "\n",
        "# Load data\n",
        "data = load_dataset('data.json')\n",
        "\n",
        "if data:\n",
        "    print(\"Block 3: Dataset berhasil dimuat\")\n",
        "    print(f\"Jumlah intent: {len(data['intents'])}\")\n",
        "\n",
        "    # contoh data\n",
        "    for i, intent in enumerate(data['intents'][:5]):\n",
        "        print(f\"Intent {i+1}: {intent['tag']}\")\n",
        "        print(f\"Patterns: {len(intent['patterns'])}\")\n",
        "        print(f\"Responses: {len(intent['responses'])}\")\n",
        "        print()\n",
        "else:\n",
        "    print(\"Block 3: Gagal memuat dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NM7vO1jPNeq2",
        "outputId": "09bc771d-dbf9-4034-f503-0222b5f9697e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Block 4: Data preprocessing berhasil\n",
            "Total training samples: 402\n",
            "Unique labels: 191\n",
            "Contoh preprocessed text: explain data abstraction\n"
          ]
        }
      ],
      "source": [
        "# Block 4: Preprocess data untuk training\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Preprocess teks dengan tokenisasi, stopword removal, dan lemmatization\"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Tokenisasi\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # Remove stopwords dan lemmatization\n",
        "    processed_tokens = [lemmatizer.lemmatize(token) for token in tokens\n",
        "                       if token.isalnum() and token not in stop_words]\n",
        "\n",
        "    return ' '.join(processed_tokens)\n",
        "\n",
        "def prepare_training_data(data):\n",
        "    \"\"\"Siapkan data untuk training embedding model\"\"\"\n",
        "    training_texts = []\n",
        "    labels = []\n",
        "\n",
        "    for intent in data['intents']:\n",
        "        tag = intent['tag']\n",
        "        for pattern in intent['patterns']:\n",
        "            processed_pattern = preprocess_text(pattern)\n",
        "            training_texts.append(processed_pattern)\n",
        "            labels.append(tag)\n",
        "\n",
        "    return training_texts, labels\n",
        "\n",
        "if data:\n",
        "    training_texts, labels = prepare_training_data(data)\n",
        "    print(\"Block 4: Data preprocessing berhasil\")\n",
        "    print(f\"Total training samples: {len(training_texts)}\")\n",
        "    print(f\"Unique labels: {len(set(labels))}\")\n",
        "    print(f\"Contoh preprocessed text: {training_texts[0]}\")\n",
        "else:\n",
        "    print(\"Block 4: Preprocessing gagal karena data tidak tersedia\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVjyCpesQb_4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304,
          "referenced_widgets": [
            "fd6b8f8d787149d9a5684545be34d497",
            "5dc0ef0ba10b4b619839d9313a8885ae",
            "92a2efe08ae547b4bf4346d2cceb274b",
            "a3a331224c4c451087f1934f6ae875df",
            "0ad90c410ab0499ebb4e5a5153a1a8a6",
            "fdfe326358d547e2b72dc75307723cb9",
            "b26904cdbe6f426caa7c1f67f61f7c82",
            "c227bfbd2c434667ad21d1082983a8e4",
            "a615eceb06cd4945a7a43920083251c6",
            "62d1acc59f28462aa1bcb4fb5dbdd2fd",
            "e24e96c626e24ae8a043af05bf1edc5f"
          ]
        },
        "id": "Pqyc6s52Nrit",
        "outputId": "3bd2040f-b055-4df1-c8f4-b176a0e159c8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model all-MiniLM-L6-v2 berhasil didownload dan dimuat\n",
            "Training examples dibuat: 385\n",
            "Memulai fine-tuning model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fd6b8f8d787149d9a5684545be34d497",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 00:11, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fine-tuning selesai\n",
            "Fine-tuned model berhasil disimpan\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_b4b6ecee-fa74-43aa-b6b7-f4b1fbf586c3\", \"best_embedding_model.zip\", 83592361)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model berhasil di-download dalam format zip\n",
            "Block 5: Model embedding berhasil di-fine-tune dan disave\n",
            "Shape embedding: (402, 384)\n",
            "Fine-tuned model disimpan di: best_embedding_model/\n"
          ]
        }
      ],
      "source": [
        "# Block 5: Fine-tuning model embedding dan save model\n",
        "\n",
        "def create_training_examples(training_texts, labels):\n",
        "    \"\"\"Buat training examples untuk fine-tuning\"\"\"\n",
        "    examples = []\n",
        "\n",
        "    # Buat positive pairs (teks dengan label yang sama)\n",
        "    label_groups = {}\n",
        "    for text, label in zip(training_texts, labels):\n",
        "        if label not in label_groups:\n",
        "            label_groups[label] = []\n",
        "        label_groups[label].append(text)\n",
        "\n",
        "    # Buat positive pairs\n",
        "    for label, texts in label_groups.items():\n",
        "        for i in range(len(texts)):\n",
        "            for j in range(i + 1, len(texts)):\n",
        "                examples.append(InputExample(texts=[texts[i], texts[j]], label=1.0))\n",
        "\n",
        "    # Buat negative pairs (teks dengan label berbeda)\n",
        "    labels_list = list(label_groups.keys())\n",
        "    for _ in range(len(examples) // 2):  # Setengah dari positive pairs\n",
        "        label1, label2 = random.sample(labels_list, 2)\n",
        "        text1 = random.choice(label_groups[label1])\n",
        "        text2 = random.choice(label_groups[label2])\n",
        "        examples.append(InputExample(texts=[text1, text2], label=0.0))\n",
        "\n",
        "    return examples\n",
        "\n",
        "def fine_tune_embedding_model(training_texts, labels):\n",
        "    \"\"\"Fine-tune all-MiniLM model dengan data domain spesifik\"\"\"\n",
        "    # Download dan load pre-trained all-MiniLM model\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    print(\"Model all-MiniLM-L6-v2 berhasil didownload dan dimuat\")\n",
        "\n",
        "    # Buat training examples\n",
        "    train_examples = create_training_examples(training_texts, labels)\n",
        "    print(f\"Training examples dibuat: {len(train_examples)}\")\n",
        "\n",
        "    # Buat DataLoader\n",
        "    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
        "\n",
        "    # Define loss function\n",
        "    train_loss = losses.CosineSimilarityLoss(model)\n",
        "\n",
        "    print(\"Memulai fine-tuning model...\")\n",
        "\n",
        "    # Fine-tune model\n",
        "    model.fit(\n",
        "        train_objectives=[(train_dataloader, train_loss)],\n",
        "        epochs=10,\n",
        "        warmup_steps=100,\n",
        "        show_progress_bar=True\n",
        "    )\n",
        "\n",
        "    print(\"Fine-tuning selesai\")\n",
        "\n",
        "    # Generate embeddings dengan model yang sudah di-fine-tune\n",
        "    embeddings = model.encode(training_texts)\n",
        "\n",
        "    # Save fine-tuned model\n",
        "    model.save('best_embedding_model')\n",
        "    print(\"Fine-tuned model berhasil disimpan\")\n",
        "\n",
        "    # Download model dari Colab\n",
        "    from google.colab import files\n",
        "    import shutil\n",
        "    import zipfile\n",
        "\n",
        "    # Zip folder model\n",
        "    shutil.make_archive('best_embedding_model', 'zip', 'best_embedding_model')\n",
        "\n",
        "    # Download zip file\n",
        "    files.download('best_embedding_model.zip')\n",
        "    print(\"Model berhasil di-download dalam format zip\")\n",
        "\n",
        "    return model, embeddings\n",
        "\n",
        "if 'data' in locals() and 'training_texts' in locals():\n",
        "    if data and training_texts:\n",
        "        embedding_model, training_embeddings = fine_tune_embedding_model(training_texts, labels)\n",
        "        print(\"Block 5: Model embedding berhasil di-fine-tune dan disave\")\n",
        "        print(f\"Shape embedding: {training_embeddings.shape}\")\n",
        "        print(f\"Fine-tuned model disimpan di: best_embedding_model/\")\n",
        "    else:\n",
        "        print(\"Block 5: Fine-tuning gagal karena data tidak tersedia\")\n",
        "else:\n",
        "    print(\"Block 5: Variabel 'data' atau 'training_texts' tidak ditemukan\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYBat1piN7IB",
        "outputId": "aa8bd315-b9e4-4be9-e276-159c66f92811"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Block 6: Testing model berhasil\n",
            "Test 1:\n",
            "  Question: What is abstraction in programming?\n",
            "  Predicted Intent: abstraction\n",
            "  Similarity Score: 0.7444\n",
            "\n",
            "Test 2:\n",
            "  Question: How to fix syntax errors?\n",
            "  Predicted Intent: error\n",
            "  Similarity Score: 0.9845\n",
            "\n",
            "Test 3:\n",
            "  Question: Why is documentation needed?\n",
            "  Predicted Intent: documentation\n",
            "  Similarity Score: 0.8747\n",
            "\n",
            "Test 4:\n",
            "  Question: What are the types of testing?\n",
            "  Predicted Intent: testing\n",
            "  Similarity Score: 0.9398\n",
            "\n",
            "Test 5:\n",
            "  Question: Explain data structurx\n",
            "  Predicted Intent: datastructure\n",
            "  Similarity Score: 0.6086\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Block 6: Test akurasi model dengan pertanyaan lain (load model dari folder)\n",
        "\n",
        "def test_model_accuracy(model_path, training_embeddings, training_texts, labels):\n",
        "    \"\"\"Load model dari folder dan test dengan pertanyaan baru\"\"\"\n",
        "    model = SentenceTransformer(model_path)\n",
        "\n",
        "    test_questions = [\n",
        "        \"What is abstraction in programming?\",\n",
        "        \"How to fix syntax errors?\",\n",
        "        \"Why is documentation needed?\",\n",
        "        \"What are the types of testing?\",\n",
        "        \"Explain data structurx\"  # tes typo\n",
        "    ]\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for question in test_questions:\n",
        "        processed_question = preprocess_text(question)\n",
        "        question_embedding = model.encode([processed_question])\n",
        "        similarities = cosine_similarity(question_embedding, training_embeddings)[0]\n",
        "        best_match_idx = np.argmax(similarities)\n",
        "        best_similarity = similarities[best_match_idx]\n",
        "        predicted_label = labels[best_match_idx]\n",
        "\n",
        "        results.append({\n",
        "            'question': question,\n",
        "            'predicted_intent': predicted_label,\n",
        "            'similarity_score': best_similarity,\n",
        "            'matched_pattern': training_texts[best_match_idx]\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "# Jalankan jika tersedia\n",
        "if data:\n",
        "    embedding_model = SentenceTransformer(\"best_embedding_model\")\n",
        "    training_embeddings = embedding_model.encode(training_texts)  # regenerasi embeddings\n",
        "    test_results = test_model_accuracy(\"best_embedding_model\", training_embeddings, training_texts, labels)\n",
        "    print(\"Block 6: Testing model berhasil\")\n",
        "\n",
        "    for i, result in enumerate(test_results, 1):\n",
        "        print(f\"Test {i}:\")\n",
        "        print(f\"  Question: {result['question']}\")\n",
        "        print(f\"  Predicted Intent: {result['predicted_intent']}\")\n",
        "        print(f\"  Similarity Score: {result['similarity_score']:.4f}\")\n",
        "        print()\n",
        "else:\n",
        "    print(\"Block 6: Testing gagal karena data tidak tersedia\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9cYiNhCTS-S",
        "outputId": "f8ffd44e-8f03-4b87-f797-24b74e4c293d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FAQ embeddings berhasil dibuat dan disimpan\n",
            "Total FAQ entries: 402\n",
            "File disimpan sebagai: faq.json\n",
            "Ukuran embedding per entry: 384\n",
            "\n",
            "Contoh FAQ entry:\n",
            "Tag: abstraction\n",
            "Original Pattern: Explain data abstraction.\n",
            "Processed Pattern: explain data abstraction\n",
            "Embedding dimension: 384\n"
          ]
        }
      ],
      "source": [
        "# Block 7: Buat embeddings dan save ke faq.json dari model yang disimpan\n",
        "\n",
        "def create_faq_embeddings(data, model_path):\n",
        "    \"\"\"Buat embeddings untuk FAQ dan simpan sebagai JSON\"\"\"\n",
        "    model = SentenceTransformer(model_path)\n",
        "    faq_data = []\n",
        "\n",
        "    for intent in data['intents']:\n",
        "        tag = intent['tag']\n",
        "        for pattern in intent['patterns']:\n",
        "            processed_pattern = preprocess_text(pattern)\n",
        "            embedding = model.encode([processed_pattern])[0]\n",
        "\n",
        "            faq_entry = {\n",
        "                'tag': tag,\n",
        "                'original_pattern': pattern,\n",
        "                'processed_pattern': processed_pattern,\n",
        "                'embedding': embedding.tolist(),\n",
        "                'responses': intent['responses']\n",
        "            }\n",
        "            faq_data.append(faq_entry)\n",
        "\n",
        "    with open('faq.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(faq_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    return faq_data\n",
        "\n",
        "# Jalankan jika data tersedia\n",
        "if data:\n",
        "    faq_embeddings = create_faq_embeddings(data, \"best_embedding_model\")\n",
        "    print(\"FAQ embeddings berhasil dibuat dan disimpan\")\n",
        "    print(f\"Total FAQ entries: {len(faq_embeddings)}\")\n",
        "    print(f\"File disimpan sebagai: faq.json\")\n",
        "    print(f\"Ukuran embedding per entry: {len(faq_embeddings[0]['embedding'])}\")\n",
        "\n",
        "    print(\"\\nContoh FAQ entry:\")\n",
        "    example_entry = faq_embeddings[0]\n",
        "    print(f\"Tag: {example_entry['tag']}\")\n",
        "    print(f\"Original Pattern: {example_entry['original_pattern']}\")\n",
        "    print(f\"Processed Pattern: {example_entry['processed_pattern']}\")\n",
        "    print(f\"Embedding dimension: {len(example_entry['embedding'])}\")\n",
        "else:\n",
        "    print(\"Block 7: Pembuatan FAQ embeddings gagal\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "g0PQg5Q-B1B8",
        "outputId": "0d03792d-273e-48a3-e48e-34f239cc53f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting googletrans==4.0.0rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0rc1)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0rc1) (2025.4.26)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0rc1)\n",
            "  Downloading hstspreload-2025.1.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0rc1) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0rc1)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0rc1)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0rc1)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0rc1)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0rc1)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0rc1)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0rc1)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0rc1)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading hstspreload-2025.1.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17396 sha256=031bfeda603e66d91ca876679c7ad49c886cb726186ae5c87603976e3c930b46\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/17/6f/66a045ea3d168826074691b4b787b8f324d3f646d755443fda\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: hyperframe\n",
            "    Found existing installation: hyperframe 6.1.0\n",
            "    Uninstalling hyperframe-6.1.0:\n",
            "      Successfully uninstalled hyperframe-6.1.0\n",
            "  Attempting uninstall: hpack\n",
            "    Found existing installation: hpack 4.1.0\n",
            "    Uninstalling hpack-4.1.0:\n",
            "      Successfully uninstalled hpack-4.1.0\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.16.0\n",
            "    Uninstalling h11-0.16.0:\n",
            "      Successfully uninstalled h11-0.16.0\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: h2\n",
            "    Found existing installation: h2 4.2.0\n",
            "    Uninstalling h2-4.2.0:\n",
            "      Successfully uninstalled h2-4.2.0\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 1.0.9\n",
            "    Uninstalling httpcore-1.0.9:\n",
            "      Successfully uninstalled httpcore-1.0.9\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "openai 1.81.0 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "google-genai 1.16.1 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.13.3 which is incompatible.\n",
            "langsmith 0.3.42 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2025.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "e4034e9b949b48fe8ac6a1cceb29114d",
              "pip_warning": {
                "packages": [
                  "chardet",
                  "h11",
                  "h2",
                  "hpack",
                  "httpcore",
                  "hyperframe",
                  "idna"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=64e4c1dc5a43bf836b8de0239807dfae28b3891b3ab7322cd15dce08e224fdc6\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ],
      "source": [
        "!pip install googletrans==4.0.0rc1\n",
        "!pip install langdetect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUmrP-3VbES9"
      },
      "outputs": [],
      "source": [
        "# Block 8: Chatbot dengan Gemini Flash (versi diringkas dan diberi komentar Bahasa Indonesia)\n",
        "\n",
        "# Konfigurasi Gemini dengan parameter creativity\n",
        "genai.configure(api_key=\"AIzaSyBgyGGOPAqHvYa4dRoS9W-A617MUCaaBR0\")\n",
        "\n",
        "# Fungsi bantu untuk preprocessing teks\n",
        "def preprocess_text(text):\n",
        "    return ' '.join(text.lower().strip().split())\n",
        "\n",
        "class RAGChatbot:\n",
        "    def __init__(self, faq_file=\"faq.json\", model_path=\"best_embedding_model\", top_k=3, max_history=10,\n",
        "                 temperature=0.7, top_p=0.9, top_k_gen=40):\n",
        "        self.top_k = top_k\n",
        "        self.max_history = max_history\n",
        "        self.chat_history = []\n",
        "\n",
        "        # Parameter untuk Gemini creativity\n",
        "        self.temperature = temperature\n",
        "        self.top_p = top_p\n",
        "        self.top_k_gen = top_k_gen\n",
        "\n",
        "        # Inisialisasi Gemini model dengan generation config\n",
        "        self.gemini_model = genai.GenerativeModel(\n",
        "            \"gemini-2.0-flash\",\n",
        "            generation_config=genai.types.GenerationConfig(\n",
        "                temperature=self.temperature,\n",
        "                top_p=self.top_p,\n",
        "                top_k=self.top_k_gen,\n",
        "                max_output_tokens=2048,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Inisialisasi translator\n",
        "        self.translator = Translator()\n",
        "\n",
        "        self.system_prompt = \"\"\"You are a helpful CS (Computer Science) assistant bot. Your role is to help answer questions related to computer science concepts based ONLY on the provided context.\n",
        "\n",
        "RULES:\n",
        "1. Answer in english by default.\n",
        "2. Answer ONLY based on the provided context from the knowledge base.\n",
        "3. You can respond to greetings (hi, hello, thanks, goodbye) in a friendly way, you can also answer about who you are.\n",
        "4. For CS questions NOT covered in the context, politely say you don't have information about that specific topic\n",
        "6. Be concise but informative\n",
        "7. If the context doesn't contain relevant information for the question, admit you don't know, and ask the question again.\n",
        "8. Please explain it further in new paragraph after, relevant to the answer.\n",
        "9. If user ask you to tell them more about the relevant context, you're allow to do it with your AI model.\n",
        "\n",
        "Remember: You are limited to the knowledge provided in the context. Do not make up information.\"\"\"\n",
        "\n",
        "        # Load FAQ embeddings dan model embedding\n",
        "        self.faq_data = self._load_json(faq_file)\n",
        "        self.embedding_model = SentenceTransformer(model_path)\n",
        "        self.embeddings_matrix = np.array([entry['embedding'] for entry in self.faq_data])\n",
        "\n",
        "    def _load_json(self, file_path):\n",
        "        \"\"\"Load data JSON dan tangani error jika file tidak ditemukan\"\"\"\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "                print(f\"Loaded {len(data)} entries from {file_path}\")\n",
        "                return data\n",
        "        except (FileNotFoundError, json.JSONDecodeError):\n",
        "            print(f\"Error loading {file_path}.\")\n",
        "            return []\n",
        "\n",
        "    def detect_language(self, text):\n",
        "        \"\"\"Deteksi bahasa dari input text\"\"\"\n",
        "        try:\n",
        "            detected_lang = detect(text)\n",
        "            print(f\"Detected language: {detected_lang}\")\n",
        "            return detected_lang\n",
        "        except:\n",
        "            print(\"Language detection failed, assuming English\")\n",
        "            return 'en'\n",
        "\n",
        "    def translate_to_english(self, text, source_lang):\n",
        "        \"\"\"Translate text ke bahasa Inggris jika bukan bahasa Inggris\"\"\"\n",
        "        if source_lang == 'en':\n",
        "            return text\n",
        "\n",
        "        try:\n",
        "            translated = self.translator.translate(text, src=source_lang, dest='en')\n",
        "            print(f\"Translated to English: {translated.text}\")\n",
        "            return translated.text\n",
        "        except Exception as e:\n",
        "            print(f\"Translation error: {e}\")\n",
        "            return text\n",
        "\n",
        "    def translate_from_english(self, text, target_lang):\n",
        "        \"\"\"Translate response dari bahasa Inggris ke bahasa target\"\"\"\n",
        "        if target_lang == 'en':\n",
        "            return text\n",
        "\n",
        "        try:\n",
        "            translated = self.translator.translate(text, src='en', dest=target_lang)\n",
        "            print(f\"Translated to {target_lang}: {translated.text}\")\n",
        "            return translated.text\n",
        "        except Exception as e:\n",
        "            print(f\"Translation error: {e}\")\n",
        "            return text\n",
        "\n",
        "    def _search_context(self, query):\n",
        "        \"\"\"\n",
        "        Mencari context relevan untuk query user menggunakan cosine similarity.\n",
        "        Mengembalikan list context dengan skor similarity.\n",
        "        \"\"\"\n",
        "        query_embedding = self.embedding_model.encode([preprocess_text(query)])\n",
        "        similarities = cosine_similarity(query_embedding, self.embeddings_matrix)[0]\n",
        "        top_indices = np.argsort(similarities)[-self.top_k:][::-1]\n",
        "\n",
        "        return [{\n",
        "            'tag': self.faq_data[idx]['tag'],\n",
        "            'pattern': self.faq_data[idx]['original_pattern'],\n",
        "            'responses': self.faq_data[idx]['responses'],\n",
        "            'similarity': similarities[idx]\n",
        "        } for idx in top_indices]\n",
        "\n",
        "    def _format_context(self, contexts):\n",
        "        \"\"\"Format context yang relevan untuk dimasukkan ke prompt Gemini\"\"\"\n",
        "        if not contexts:\n",
        "            return \"No relevant context found.\"\n",
        "\n",
        "        lines = [\"KNOWLEDGE BASE CONTEXT:\"]\n",
        "        for i, ctx in enumerate(contexts, 1):\n",
        "            lines.append(\n",
        "                f\"\\n{i}. Topic: {ctx['tag']}\\n\"\n",
        "                f\"   Question Pattern: {ctx['pattern']}\\n\"\n",
        "                f\"   Responses: {'; '.join(ctx['responses'])}\\n\"\n",
        "                f\"   Relevance Score: {ctx['similarity']:.4f}\"\n",
        "            )\n",
        "        return '\\n'.join(lines)\n",
        "\n",
        "    def _format_history(self):\n",
        "        \"\"\"Format chat history untuk konteks prompt\"\"\"\n",
        "        if not self.chat_history:\n",
        "            return \"\"\n",
        "        return \"\\nCHAT HISTORY:\\n\" + '\\n\\n'.join(\n",
        "            f\"User: {h['user']}\\nAssistant: {h['assistant']}\"\n",
        "            for h in self.chat_history[-self.max_history:]\n",
        "        )\n",
        "\n",
        "    def _update_history(self, user, assistant):\n",
        "        \"\"\"Tambahkan percakapan ke history dan batasi ke max_history\"\"\"\n",
        "        self.chat_history.append({'user': user, 'assistant': assistant})\n",
        "        if len(self.chat_history) > self.max_history:\n",
        "            self.chat_history = self.chat_history[-self.max_history:]\n",
        "\n",
        "    def generate_response(self, user_query):\n",
        "        \"\"\"\n",
        "        Fungsi utama untuk menghasilkan respon dengan multilingual support:\n",
        "        1. Deteksi bahasa input\n",
        "        2. Translate ke English jika perlu\n",
        "        3. Semantic search\n",
        "        4. Generate response dengan Gemini\n",
        "        5. Translate response kembali ke bahasa input\n",
        "        \"\"\"\n",
        "        # Step 1: Deteksi bahasa input\n",
        "        detected_lang = self.detect_language(user_query)\n",
        "\n",
        "        # Step 2: Translate ke English jika bukan English\n",
        "        english_query = self.translate_to_english(user_query, detected_lang)\n",
        "\n",
        "        # Step 3: Semantic search dengan query English\n",
        "        print(f\"Mencari context relevan (top-{self.top_k})...\")\n",
        "        contexts = self._search_context(english_query)\n",
        "\n",
        "        # Step 4: Format prompt dengan instruksi bahasa\n",
        "        lang_instruction = f\"IMPORTANT: Respond ONLY in the same language as this original user question: '{user_query}'. Do not provide multiple language versions or translations.\"\n",
        "\n",
        "        prompt = f\"\"\"{self.system_prompt}\n",
        "\n",
        "{self._format_context(contexts)}\n",
        "\n",
        "{self._format_history()}\n",
        "\n",
        "{lang_instruction}\n",
        "\n",
        "Current User Question: {user_query}\n",
        "\n",
        "Please provide a helpful response based on the context above.\"\"\"\n",
        "\n",
        "        try:\n",
        "            print(\"CS Helper bot is answering...\")\n",
        "            response = self.gemini_model.generate_content(prompt)\n",
        "            bot_response = response.text.strip()\n",
        "\n",
        "            # Update history dengan bahasa asli user\n",
        "            self._update_history(user_query, bot_response)\n",
        "            return bot_response, contexts, detected_lang\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Maaf, terjadi error: {str(e)}\"\n",
        "            # Translate error message ke bahasa user jika perlu\n",
        "            if detected_lang != 'en':\n",
        "                error_msg = self.translate_from_english(\"Sorry, an error occurred: \" + str(e), detected_lang)\n",
        "            return error_msg, contexts, detected_lang\n",
        "\n",
        "    def chat(self, user_query):\n",
        "        \"\"\"\n",
        "        Fungsi interaktif chatbot dengan multilingual support:\n",
        "        - Terima pertanyaan user dalam bahasa apapun\n",
        "        - Proses dengan alur multilingual\n",
        "        - Cetak hasil dan context yang ditemukan\n",
        "        \"\"\"\n",
        "        print(f\"\\nUser: {user_query}\")\n",
        "        bot_response, contexts, detected_lang = self.generate_response(user_query)\n",
        "        print(f\"Bot: {bot_response}\")\n",
        "\n",
        "        # Debug: tampilkan context yang ditemukan\n",
        "        print(f\"\\n[DEBUG] Detected Language: {detected_lang}\")\n",
        "        print(\"[DEBUG] Context yang ditemukan:\")\n",
        "        for i, ctx in enumerate(contexts, 1):\n",
        "            print(f\"  {i}. {ctx['tag']} (similarity: {ctx['similarity']:.4f})\")\n",
        "\n",
        "        return bot_response\n",
        "\n",
        "    # Fungsi untuk clear chat history\n",
        "    def clear_history(self):\n",
        "        self.chat_history = []\n",
        "        print(\"Chat history sudah dihapus.\")\n",
        "\n",
        "    # Fungsi untuk melihat seluruh chat history\n",
        "    def show_history(self):\n",
        "        if not self.chat_history:\n",
        "            print(\"Belum ada chat history.\")\n",
        "            return\n",
        "        print(\"\\n=== CHAT HISTORY ===\")\n",
        "        for i, h in enumerate(self.chat_history, 1):\n",
        "            print(f\"{i}. User: {h['user']}\")\n",
        "            print(f\"   Bot: {h['assistant']}\\n\")\n",
        "\n",
        "    # Fungsi untuk melihat bahasa yang didukung\n",
        "    def show_supported_languages(self):\n",
        "        \"\"\"Menampilkan daftar bahasa yang didukung oleh Google Translate\"\"\"\n",
        "        print(\"\\n=== SUPPORTED LANGUAGES ===\")\n",
        "        for code, name in LANGUAGES.items():\n",
        "            print(f\"{code}: {name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 902
        },
        "id": "lEUNbL4m7sXA",
        "outputId": "d22fcaf1-09c2-4c46-e1f0-f2ed3f9be667"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "INITIALIZING RAG CHATBOT (VERSI INTERAKTIF)\n",
            "============================================================\n",
            "Loaded 402 entries from faq.json\n",
            "\n",
            "Chatbot siap digunakan!\n",
            "\n",
            "Ketik 'exit' untuk berhenti.\n",
            "\n",
            "You: ajarkan saya tentang makan\n",
            "\n",
            "User: ajarkan saya tentang makan\n",
            "Detected language: id\n",
            "Translated to English: Teach me about eating\n",
            "Mencari context relevan (top-3)...\n",
            "CS Helper bot is answering...\n",
            "Bot: Maaf, saya tidak memiliki informasi tentang topik \"makan\". Bisakah Anda mengajukan pertanyaan lain?\n",
            "\n",
            "[DEBUG] Detected Language: id\n",
            "[DEBUG] Context yang ditemukan:\n",
            "  1. mealy_and_moore_machines (similarity: 0.4368)\n",
            "  2. mealy_and_moore_machines (similarity: 0.4346)\n",
            "  3. threads (similarity: 0.2128)\n",
            "----------------------------------------\n",
            "You: apkah benar?\n",
            "\n",
            "User: apkah benar?\n",
            "Detected language: id\n",
            "Translated to English: Is it true?\n",
            "Mencari context relevan (top-3)...\n",
            "CS Helper bot is answering...\n",
            "Bot: Maaf, saya tidak memiliki informasi tentang topik tersebut. Bisakah Anda mengajukan pertanyaan lain?\n",
            "\n",
            "[DEBUG] Detected Language: id\n",
            "[DEBUG] Context yang ditemukan:\n",
            "  1. truth tables (similarity: 0.3826)\n",
            "  2. first_order_logic (similarity: 0.3463)\n",
            "  3. first_order_logic (similarity: 0.3431)\n",
            "----------------------------------------\n",
            "You: exit\n",
            "Chatbot: Goodbye! Terima kasih sudah menggunakan chatbot ini.\n",
            "\n",
            "=== FINAL CHAT HISTORY ===\n",
            "\n",
            "=== CHAT HISTORY ===\n",
            "1. User: ajarkan saya tentang makan\n",
            "   Bot: Maaf, saya tidak memiliki informasi tentang topik \"makan\". Bisakah Anda mengajukan pertanyaan lain?\n",
            "\n",
            "2. User: apkah benar?\n",
            "   Bot: Maaf, saya tidak memiliki informasi tentang topik tersebut. Bisakah Anda mengajukan pertanyaan lain?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"INITIALIZING RAG CHATBOT (VERSI INTERAKTIF)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Hanya initialize chatbot jika data dari block sebelumnya sudah siap\n",
        "if data:  # variabel 'data' dari block sebelumnya\n",
        "    chatbot = RAGChatbot(\n",
        "        faq_file=\"faq.json\",\n",
        "        model_path=\"best_embedding_model\",\n",
        "        top_k=3,\n",
        "        max_history=10,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        top_k_gen=40\n",
        "    )\n",
        "    print(\"\\nChatbot siap digunakan!\")\n",
        "\n",
        "    # Ganti test_queries dengan loop interaktif\n",
        "    print(\"\\nKetik 'exit' untuk berhenti.\\n\")\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.strip().lower() == 'exit':\n",
        "            print(\"Chatbot: Goodbye! Terima kasih sudah menggunakan chatbot ini.\")\n",
        "            break\n",
        "        chatbot.chat(user_input)\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "    print(\"\\n=== FINAL CHAT HISTORY ===\")\n",
        "    chatbot.show_history()\n",
        "\n",
        "else:\n",
        "    print(\"Tidak bisa inisialisasi chatbot: data tidak tersedia.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0ad90c410ab0499ebb4e5a5153a1a8a6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "5dc0ef0ba10b4b619839d9313a8885ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fdfe326358d547e2b72dc75307723cb9",
            "placeholder": "​",
            "style": "IPY_MODEL_b26904cdbe6f426caa7c1f67f61f7c82",
            "value": "Computing widget examples:   0%"
          }
        },
        "62d1acc59f28462aa1bcb4fb5dbdd2fd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92a2efe08ae547b4bf4346d2cceb274b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c227bfbd2c434667ad21d1082983a8e4",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a615eceb06cd4945a7a43920083251c6",
            "value": 1
          }
        },
        "a3a331224c4c451087f1934f6ae875df": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62d1acc59f28462aa1bcb4fb5dbdd2fd",
            "placeholder": "​",
            "style": "IPY_MODEL_e24e96c626e24ae8a043af05bf1edc5f",
            "value": " 0/1 [00:00&lt;?, ?example/s]"
          }
        },
        "a615eceb06cd4945a7a43920083251c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b26904cdbe6f426caa7c1f67f61f7c82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c227bfbd2c434667ad21d1082983a8e4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e24e96c626e24ae8a043af05bf1edc5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd6b8f8d787149d9a5684545be34d497": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5dc0ef0ba10b4b619839d9313a8885ae",
              "IPY_MODEL_92a2efe08ae547b4bf4346d2cceb274b",
              "IPY_MODEL_a3a331224c4c451087f1934f6ae875df"
            ],
            "layout": "IPY_MODEL_0ad90c410ab0499ebb4e5a5153a1a8a6"
          }
        },
        "fdfe326358d547e2b72dc75307723cb9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
